# 클라우드용 프로그래밍 모델

분산 프로그래밍 모델은 순차 알고리즘을 분산 시스템에서 실행할 수 있는 분산 프로그램으로 변환할 수 있도록 지원한다. 아키텍처/하드웨어 디테일을 추상화하고, 자동으로 병렬 처리와 분산하고, 내결함성을 투명하게 지원하는 모델이 편리한 모델로 간주된다.

그러나 모델의 효율성은 기본 기술의 효율성에 의해 결정된다. 분산 시스템에서 실행되는 분산 프로그램의 필수 요구사항은 네트워크로 연결된 여러 리소스에서 구성 요소 작업을 조정할 수 있는 통신 메커니즘이다. 기존의 messaging passing과 shared memory이 이를 충족시킨다. 클라우드 환경에서 분산 프로그램의 추가적인 과제는 분산 분석 엔진으로 동작할 때, 자동으로 태스크를 병렬 처리하고 분산할 수 있으며, 내결함성을 가진 정교한 프로그래밍 모델을 구현하는 것이다.

## 공유 메모리 모델

공유 메모리 모델의 핵심은 모든 태스크가 애플리케이션 분산 메모리 공간의 모든 위치에 접근할 수 있음을 의미한다. 따라서 태스크는 스레드가 프로세스의 주소 공간을 공유하는 것처럼 분산 메모리 메모리 공간에서 읽고 쓰는 것으로 암시적으로 통신한다. 따라서 공유 메모리 모델은 분산 프로그램이 다양한 태스크가 읽기/쓰기작업을 수행할 수 있는 순서를 제어하는 데 사용해야 하는 동기화 메커니즘, 특히 여러 태스크가 동시에 공유 데이터 위치에 읽고 쓸 수 없어야 한다. 이는 `락`, `세마포어`, `배리어`로 구현한다.

`세마포어`는 두 개의 병렬/분산 태스크를 포함하는 지점 간 동기화 매커니즘으로, **post**와 **wait** 연산으로 구현된다. Post는 데이터가 생성되었음을 알리고, wait는 post 작업이 데이터를 계속 사용할 수 있음을 신호로 알릴 때까지 차단된다. `락`은 한 번에 하나의 태스크만 액세스 할 수 있도록 하는 매커니즘이다. 마지막으로 `배리어`는 다른 모든 태스크가 해당 지점에 도달할 때까지 태스크를 계속할 수 없는 지점을 말한다.

한 순차 프로그램이 있다. 배열 a, b, c가 있고, b와 c의 같은 인덱스에 해당하는 값을 합하여 a에 할당한다. 그 후 a 배열에서 양수인 요소들만 sum 변수에 합하여 출력한다.

```s
for (i = 0; i < 8; i++)
  a[i] = b[i] + c[i]
sum = 0;
for (i = 0; i< 8; i++)
  if (a[i] > 0)
    sum = sum + a[i]
Print sum;
```

이를 분산 프로그램으로 변환한다면, 두 개의 태스크만 가정하고 두 태스크 간 작업을 균일하게 분할한다. 모든 태스크에 대해 배열을 올바르게 인덱싱하고, 데이터를 가져오고, 지정된 알고리즘을 적용하기 위해 start 및 end 변수가 지정된다. sum 변수는 크리티컬 섹션이므로 락으로 보호된다. 또한 다른 모든 태스크에서 작업을 완료하기 전에 어떤 태스크도 sum을 출력할 수 없도록 print 앞에 배리어를 둔다.

```c
begin parallel  // spawn a child thread
private int start_inter, end_iter, i;
shared int local_iter=4, sum=0;
shared double sum=0.0, a[], b[], c[];
shared lock_type mylock;
start_iter = getid() * local_iter;
end_iter = start_iter + local_iter;
for (i=start_iter; i<end_iter; i++)
  a[i] = b[i] + c[i];
barrier;
for (i=start_iter; i<end_iter; i++)
  if (a[i] > 0) {
    lock(mylock);
      sum = sum + a[i];
    unlock(mylock);
  }
barrier;
end parallel; // kill the child thread
Print sum;
```

프로그램에 표시된 것처럼 두 태스크 간의 통신은 암시적이며, 동기화는 명시적이다. 

## 메세지 전달 프로그래밍 모델

메세지 전달 프로그래밍 모델에서는 분산 태스크는 메세지를 주고받으며 통신한며, 메모리 주소 공간을 공유하지 않는다. 명시적으로 메세지를 주고받기 때문에 메세지로 인한 오버헤드가 발생한다. 명시적 메세지 교환은 이런 오버헤드를 분산시켜 통신하는 태스크의 작업 순서를 암시적으로 동기화한다.

공유 메모리 모델에서 사용했던 프로그램을 여기서 똑같이 적용해보자. 처음 수행할 작업은 두 태스크 간 작업을 균등하게 분할하기 위해 명시적으로 배열의 일부를 다른 태스크로 전송한다. 두 번째 태스크는 이를 수신하고 로컬 합산을 수행한다. 로컬 합산이 완료되면 주 작업으로 다시 전송한다. 주 작업은 나누어진 태스크의 결과를 데이터 수신으로 받고 로컬에서 합산하여 출력한다. 모든 전송 작업에 대해 수신 작업이 있으며, 명시적 동기화가 필요하지 않다. 또한 메세지 전달 프로그래밍 모델을 사용하기 위해 기본 분산 프로그램이 반드시 지원될 필요는 없다. 이런 모델은 `MPI(Message Passing Interface)`에서 제공하고, 이는 메세지 전달 프로그램을 작성하기 위한 표준 전달 라이브러리이다. 널리 사용되는 고성능 MPI 구현은 **MPICH**이다.

프로그래머는 데이터 레이아웃 또는 통신 방식을 걱정하지 않아도 되기 때문에 공유 메모리 프로그램은 초기에 쉽게 개발할 수 있다. 또한 공유 메모리 모델은 순차 프로그래밍과 유사하다. 반면, 메세지 전달 프로그램에서는 태스크 간 데이터를 분할하고, 데이터를 수집/통신하고, 명시적 메세징을 통해 결과를 집계하는 방법을 고려하기 위해 관점을 전환할 필요가 있다.

두 모델을 비교 정리하자면 다음과 같다.

| 양상 | 공유 메모리 모델 | 메세지 전달 모델 |
| --- | ------------ | ------------ |
| 통신 | 암시적         | 명시적        |
| 동기화 | 명시적       | 암시적         |
| 하드웨어 지원 | 일반적으로 필수 | 필요하지 않음 | 
| 초기 개발 활동 | 더 낮음 | 더 높음 |
| 확장 시 조정 활동 | 더 높음 | 더 낮음 |